# execeute this file as flume-ng agent --conf (conf directory) -f (file name.conf) -n a1 -Dflume.root.logger=INFO,consoleD
# "--conf" denotes the directory name, "-f" has the file name and "-n" is the running configuration(logger in this case)
# r1 pulls a new-line seperated text and generates a Flume event 
# Single Node In and Out


# Configurations
a1.sources = r1
a1.channels = c1
a1.sinks = spark


# Source description 

#a1.sources.r1.type = netcat
#a1.sources.r1.bind = localhost
#a1.sources.r1.port = 44444 #Default port number 

a1.sources.r1.type = exec
a1.sources.r1.command = tail -f /tmp/a.out # read/tail the last lines  from the directory /tmp file name a.out
a1.sources.r1.channels = c1 #Default port number 



# Sink Description
# Build a custom Sink for k1 to PULL data from the sinks

a1.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
a1.sinks.spark.hostname = localhost 
#a1.sinks.spark.hostname = 127.0.0.1 
a1.sinks.spark.port = 7782 
 #port to listen on for connection from Spark>
a1.sinks.spark.channels = memoryChannel


# c1 Description 
# Basic channels to buff the log 

a1.channels.c1.type = memory
a1.channels.c1.capacty = 1000
a1.channels.c1.trasactionCapacity = 100


# Binding Source, and Sink to the c1

a1.sources.r1.channels = c1
a1.sinks.spark.channel = c1
